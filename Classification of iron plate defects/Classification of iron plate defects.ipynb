{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b93f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing necessary libraries\n",
    "!pip install pandas\n",
    "!pip install tensorflow\n",
    "!pip install keras\n",
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45b08a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd44d256",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading train images info from data/train.csv\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "image_names = train_df['ImageId'].values\n",
    "labels = train_df['ClassId'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcf9bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading train images\n",
    "images = []\n",
    "for image_name in image_names:\n",
    "    image_path = f'data/train_images/{image_name}'\n",
    "    image = cv2.imread(image_path)\n",
    "    img = cv2.resize(image,(64,64)) #Resize images to (64,64)\n",
    "    images.append(img) #Add image to \"images\" list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1bb31f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spliting the data into training and validation (80% for train and 20% for test)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(images, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee3b5c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the images and labels to numpy arrays\n",
    "train_images = np.array(train_images)\n",
    "val_images = np.array(val_images)\n",
    "train_labels = to_categorical(train_labels - 1)  #Convert labels to one-hot encoding\n",
    "val_labels = to_categorical(val_labels - 1)  #Convert labels to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf35fcb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec739e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60402524",
   "metadata": {},
   "source": [
    "# Desired model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd7218e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify the height, width and number of image channels\n",
    "image_height = 64\n",
    "image_width = 64\n",
    "num_channels = 3\n",
    "\n",
    "#Specify the number of labels\n",
    "num_classes = 4\n",
    "\n",
    "#CNN model with\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=(image_height, image_width, num_channels)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee583c8c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "178/178 [==============================] - 7s 37ms/step - loss: 0.7496 - accuracy: 0.7225 - val_loss: 0.6386 - val_accuracy: 0.7435\n",
      "Epoch 2/10\n",
      "178/178 [==============================] - 7s 36ms/step - loss: 0.7200 - accuracy: 0.7274 - val_loss: 0.7104 - val_accuracy: 0.7301\n",
      "Epoch 3/10\n",
      "178/178 [==============================] - 7s 36ms/step - loss: 0.7061 - accuracy: 0.7297 - val_loss: 0.6657 - val_accuracy: 0.7470\n",
      "Epoch 4/10\n",
      "178/178 [==============================] - 6s 36ms/step - loss: 0.6700 - accuracy: 0.7451 - val_loss: 0.7130 - val_accuracy: 0.7393\n",
      "Epoch 5/10\n",
      "178/178 [==============================] - 6s 36ms/step - loss: 0.6864 - accuracy: 0.7354 - val_loss: 0.6400 - val_accuracy: 0.7308\n",
      "Epoch 6/10\n",
      "178/178 [==============================] - 6s 36ms/step - loss: 0.6454 - accuracy: 0.7415 - val_loss: 0.6006 - val_accuracy: 0.7541\n",
      "Epoch 7/10\n",
      "178/178 [==============================] - 6s 36ms/step - loss: 0.6678 - accuracy: 0.7414 - val_loss: 0.6857 - val_accuracy: 0.7449\n",
      "Epoch 8/10\n",
      "178/178 [==============================] - 7s 36ms/step - loss: 0.6638 - accuracy: 0.7387 - val_loss: 0.5973 - val_accuracy: 0.7449\n",
      "Epoch 9/10\n",
      "178/178 [==============================] - 7s 37ms/step - loss: 0.6884 - accuracy: 0.7350 - val_loss: 0.6658 - val_accuracy: 0.7378\n",
      "Epoch 10/10\n",
      "178/178 [==============================] - 6s 36ms/step - loss: 0.6530 - accuracy: 0.7458 - val_loss: 0.6868 - val_accuracy: 0.7449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e98ba0ee90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, batch_size=32, epochs=10, validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96530ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d381f79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41d12cc9",
   "metadata": {},
   "source": [
    "# using VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c79a8c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb991311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 110s 2us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(image_height, image_width, num_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d32f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2996180",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7b74af4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "178/178 [==============================] - 26s 137ms/step - loss: 0.3896 - accuracy: 0.8460 - val_loss: 0.8662 - val_accuracy: 0.8013\n",
      "Epoch 2/10\n",
      "178/178 [==============================] - 24s 134ms/step - loss: 0.3245 - accuracy: 0.8813 - val_loss: 0.9594 - val_accuracy: 0.8118\n",
      "Epoch 3/10\n",
      "178/178 [==============================] - 24s 135ms/step - loss: 0.2816 - accuracy: 0.8876 - val_loss: 0.8731 - val_accuracy: 0.8111\n",
      "Epoch 4/10\n",
      "178/178 [==============================] - 24s 135ms/step - loss: 0.2584 - accuracy: 0.8915 - val_loss: 0.8170 - val_accuracy: 0.8111\n",
      "Epoch 5/10\n",
      "178/178 [==============================] - 24s 135ms/step - loss: 0.2471 - accuracy: 0.8945 - val_loss: 0.7987 - val_accuracy: 0.8097\n",
      "Epoch 6/10\n",
      "178/178 [==============================] - 24s 135ms/step - loss: 0.2457 - accuracy: 0.8973 - val_loss: 0.8390 - val_accuracy: 0.8147\n",
      "Epoch 7/10\n",
      "178/178 [==============================] - 24s 135ms/step - loss: 0.2301 - accuracy: 0.8964 - val_loss: 0.7724 - val_accuracy: 0.8111\n",
      "Epoch 8/10\n",
      "178/178 [==============================] - 24s 135ms/step - loss: 0.2165 - accuracy: 0.9010 - val_loss: 0.7354 - val_accuracy: 0.8090\n",
      "Epoch 9/10\n",
      "178/178 [==============================] - 24s 135ms/step - loss: 0.2158 - accuracy: 0.9045 - val_loss: 0.7348 - val_accuracy: 0.8048\n",
      "Epoch 10/10\n",
      "178/178 [==============================] - 24s 135ms/step - loss: 0.1983 - accuracy: 0.9103 - val_loss: 0.7763 - val_accuracy: 0.8083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e9b39462d0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, batch_size=32, epochs=10, validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2823e4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605a2028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e115d59",
   "metadata": {},
   "source": [
    "# using ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d59ed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20285e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 130s 1us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(image_height, image_width, num_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b437218",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4579ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3be25c6a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "178/178 [==============================] - 27s 141ms/step - loss: 0.4004 - accuracy: 0.8552 - val_loss: 0.7669 - val_accuracy: 0.8013\n",
      "Epoch 2/10\n",
      "178/178 [==============================] - 24s 136ms/step - loss: 0.3024 - accuracy: 0.8797 - val_loss: 1.1864 - val_accuracy: 0.8034\n",
      "Epoch 3/10\n",
      "178/178 [==============================] - 24s 137ms/step - loss: 0.3277 - accuracy: 0.8813 - val_loss: 0.8747 - val_accuracy: 0.8090\n",
      "Epoch 4/10\n",
      "178/178 [==============================] - 24s 137ms/step - loss: 0.2686 - accuracy: 0.8906 - val_loss: 0.6563 - val_accuracy: 0.8076\n",
      "Epoch 5/10\n",
      "178/178 [==============================] - 24s 137ms/step - loss: 0.2565 - accuracy: 0.8945 - val_loss: 1.3891 - val_accuracy: 0.8147\n",
      "Epoch 6/10\n",
      "178/178 [==============================] - 24s 137ms/step - loss: 0.3291 - accuracy: 0.8927 - val_loss: 0.8797 - val_accuracy: 0.8048\n",
      "Epoch 7/10\n",
      "178/178 [==============================] - 24s 136ms/step - loss: 0.2561 - accuracy: 0.8989 - val_loss: 0.8773 - val_accuracy: 0.7970\n",
      "Epoch 8/10\n",
      "178/178 [==============================] - 24s 134ms/step - loss: 0.2406 - accuracy: 0.8998 - val_loss: 0.8624 - val_accuracy: 0.8069\n",
      "Epoch 9/10\n",
      "178/178 [==============================] - 24s 134ms/step - loss: 0.2349 - accuracy: 0.9013 - val_loss: 0.8386 - val_accuracy: 0.8013\n",
      "Epoch 10/10\n",
      "178/178 [==============================] - 24s 134ms/step - loss: 0.2279 - accuracy: 0.9064 - val_loss: 1.0864 - val_accuracy: 0.8034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e9c7b62890>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, batch_size=32, epochs=10, validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be799cca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92de768c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4c607c4",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6410fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the test images\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "test_image_names = test_df['ImageId'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ba189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading test images\n",
    "test_images = []\n",
    "for image_name in test_image_names:\n",
    "    image_path = f'data/test_images/{image_name}'\n",
    "    image = cv2.imread(image_path)\n",
    "    #Add image to \"test_images\" list\n",
    "    test_images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f831532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Convert the test images to numpy array\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "#Predict the test set labels\n",
    "test_predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49abc0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_labels = (np.argmax(test_predictions, axis=1)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9467cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28903242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5e21c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce54dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the saved model\n",
    "model = load_model('model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
